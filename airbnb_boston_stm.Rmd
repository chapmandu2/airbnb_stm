---
title: "AirBnB Boston"
author: "Phil Chapman"
date: "08/04/2018"
output: 
  html_document:
    number_sections: yes
    theme: cosmo
    highlight: tango
    toc: yes
    toc_depth: 3
    code_folding: show
---

# Introduction
In the analysis we use the structural topic modelling method from the R [stm package](https://www.structuraltopicmodel.com/) to analyse AirBnB listings data and identify topics that are related to the cost per night of the listing.

# Analysis set up

## Load libraries

```{r setup, message=FALSE}
library(dplyr, quietly=TRUE)
library(tidyr, quietly=TRUE)
library(readr, quietly=TRUE)
library(purrr, quietly=TRUE)
library(stm, quietly=TRUE)
library(tidytext, quietly=TRUE)
library(ggplot2, quietly=TRUE)
library(quanteda, quietly=TRUE)
```

## Load data

List the files in the data area.  These were downloaded from Kaggle at https://www.kaggle.com/airbnb/boston.

```{r}
datapath <- '~/data/boston-airbnb-open-data/'
list.files(datapath)
```

Load the data into R using the `readr` package.

```{r warnings=FALSE}
listings <- readr::read_csv(file.path(datapath, 'listings.csv'))
dim(listings)
colnames(listings)
```

# Data preparation

## Select data of interest

Only require some of the fields for later analysis.  In particular we are interested in price and accomodation description.  We also only want to analyse listings for an entire property.

```{r}
stm_df <- listings %>%
  dplyr::filter(grepl('Entire', room_type)) %>%
  dplyr::transmute(id, price=log(as.numeric(gsub('\\$', '', price))), comments=description) %>%
  dplyr::filter(!is.na(price)) 
stm_df
```

## Tokenise and remove stopwords

Tidy the comments using the tidytext 'bag of words' approach bu tokenising and then removing stopwords with an anti-join.

```{r}
tidy_comments <- stm_df %>%
  unnest_tokens(word, comments) %>%
  anti_join(stop_words)
tidy_comments
```

## Examine the number of words extracted

We can count the number of words present following tokensiation:

```{r}
tidy_comments %>%
  count(word, sort=TRUE)
```

Then extract the most common 1000 words to be used in the analysis

```{r}
common_words <- tidy_comments %>%
  count(word, sort=TRUE) %>%
  top_n(1000,n) %>% dplyr::select(-n)
common_words
```

## Create a document term frequency matrix

The tidy comments data frame can be converted into a document term frequency matrix using the `tidytext::cast_dfm` function.  The object type is a `dfm` object from the `quanteda` package.  

```{r}
comments_dfm <- tidy_comments %>%
  inner_join(common_words) %>%
  count(id, word) %>%
  arrange(id) %>%
  tidytext::cast_dfm(id, word, n)
comments_dfm
```

## Make a metadata data frame

In order to include covariates in the Structural Topic Model, we also need to create a metadata data frame.  In this case the covariate of interest is the price of the listing.  Importantly the rownames of the metadata dataframe must be the document id (ie listing id in this case).

```{r}
comments_dfm_meta <- stm_df %>%
  dplyr::group_by(id, price) %>%
  dplyr::summarise(comments=dplyr::first(comments)) %>%
  ungroup() %>%
  dplyr::filter(id %in% docnames(comments_dfm)) %>%
  dplyr::arrange(id) %>%
  as.data.frame()
rownames(comments_dfm_meta) <- comments_dfm_meta$listing_id
```

## Create an stm corpus object

Now convert the dataframes in an stm corpus object

```{r}
comments_stmc <- asSTMCorpus(comments_dfm, data=comments_dfm_meta)
```

# Topic modelling

## Identify optimal value of K

### Set up search grid

Specify the range of topic numbers that we want to search.  Since we can parallise across up to 16 cores we will look at 30 different values of K - leaving one core free.

```{r}
n_topics <- seq(4,33,1)
length(n_topics)

```

### Run the grid search

Now run the grid search over the 30 values of K specified.

```{r searchK, eval=FALSE}
topic_store <- searchK(comments_stmc$documents,
  comments_stmc$vocab,
  K = n_topics,
  data = comments_stmc$data,
  prevalence = ~ price,
  max.em.its = 20,
  cores = 15)
```

### Evaluate the models generated

The stm package includes functionality to generate some useful diagnostic plots:

```{r eval=FALSE}
plot(topic_store)
```

Not very clear in this case, probably don't have sufficient observations, but 8 looks reasonable.

```{r}
best_k <- 8
```

## Fit the topic model

Now fit the topic model with the chosen number of topics.

```{r fit-model}
topic_model <- stm(comments_stmc$documents,
  comments_stmc$vocab,
  K = best_k,
  data = comments_stmc$data,
  prevalence = ~ price,
  max.em.its = 20,
  verbose=FALSE)
```

Generate a summary of the topic model.  For each topic, the topi words according to four different metrics are generated.  Top probability is just the most common words, the other metrics use different forms of weighting to pick up words that are more exlusive to the topic.  See the stm vignette and package documentation for more information.

```{r}
summary(topic_model)
```

## Estimate the effect of price

Now estimate the effect of the price covariate on the prevalence of each topic:

```{r est-effect}
est_effect <- estimateEffect(1:best_k ~ price, topic_model, meta=comments_dfm_meta)
```


# Explore topic model

## Basic plots

### Convergence

How well did the topic model converge?

```{r}
plot(topic_model$convergence$bound, type='l')
```

### Overview of topics

Let's see a summary of the topic model.

```{r}
plot(topic_model, type='summary')
```

## Explore covariate effects

### Visualise effects

First just plot the effect of price on topic prevalence for topics 1-4:

```{r}
plot(est_effect, covariate='price', method='continuous', topics=1:4)
```

And now topics 5-8

```{r}
plot(est_effect, covariate='price', method='continuous', topics=5:8)
```

### Focus on topics 1 and 7

What are topics 1 and 7?  These seem to have a positive and negative relationship with price respectively (up vs down slope).  

```{r}
plot(est_effect, covariate='price', method='continuous', topics=c(1,7))
```

Terms like boston and appartment tend to appear in many topics, so in this case looking at FREX, score and lift is useful as we see words exclusive to these topics.

```{r}
labelTopics(topic_model, c(1,7), n=10)
```

Generate a wordcloud of most common terms for each:

```{r}
cloud(topic_model, topic=1, max.words = 10, color='red')
cloud(topic_model, topic=7, max.words = 10, color='cyan')
```

View topic probabilities for the words luxury and studio - these topics are quite specific for these terms:

```{r}
tidy(topic_model) %>% 
  dplyr::filter(term %in% c('stainless', 'station')) %>%
  ggplot(aes(x=paste('Topic', topic), y=beta, fill=term)) +
    geom_col(alpha=0.8, show.legend=FALSE) +
    facet_wrap(~term)  + 
    labs(x='topic #', y=expression(beta)) + 
    theme_bw() + theme(axis.text.x=element_text(angle=90, vjust=0.5))
```

From this analysis the topic modelling seems to have picked out topics associated with the words luxury and studio which are associated with price in the expected direction.

### Combined plot of all topics

In the plot below, the top terms for each topic are plotted, and colour represents the steepness of the slope of the regression line.

First extract topic betas with tidytext tidier.

```{r}
topic_top_terms <- tidy(topic_model) %>%
  group_by(topic) %>%
  top_n(10,beta) %>%
  ungroup()
topic_top_terms
```

Then extract slopes from the effectEstimate object:

```{r}
slope_ests <- tidyr::crossing(topic=1:best_k, rep=1:20) %>%
  mutate(slope_est = map2_dbl(topic, rep, ~est_effect$parameters[[.x]][[.y]]$est['price'])) %>%
  group_by(topic) %>%
  summarise(slope_est=mean(slope_est))
slope_ests
```

To make the plot, join the top terms and the slope estimates and then facet by topoic,.  Interpret the plot as below:

- contribution of terms to the topic shown by bar
- colour represents slope:
  - blue = more prevalent in higher priced property descriptions
  - red = more prevalent in lower priced property descriptions
  
```{r}
topic_top_terms %>%
  inner_join(slope_ests) %>%
  mutate(topic = paste0('Topic ', topic)) %>%
  ggplot(aes(term, beta, fill=slope_est)) +
    geom_col(alpha=0.8, show.legend=FALSE) +
    scale_fill_gradient2(high='red', mid='lightgray', low='blue') +
    facet_wrap(~ topic, scales='free_y') +
    coord_flip() +
    labs(x=NULL, y=expression(beta)) + theme_bw()
```

# Conclusions
Using structual topic modelling on AirBnb data we are able to identify topics that are related to high and low price listings.  For example, a topic associated with the term 'luxury' was associated with high price, and a topic associated with 'studio' was associated with low price.

# Session Info

```{r}
Sys.time()
sessionInfo()
cat(paste(readLines('/etc/docker/docker_info.txt'), '\n'))
cat(paste(readLines('/etc/docker/docker_build_history.txt'), '\n'))

```

