---
title: "AirBnB Boston"
author: "Phil Chapman"
date: "08/04/2018"
output: 
  html_document:
    number_sections: yes
    theme: cosmo
    highlight: tango
    toc: yes
    toc_depth: 3
    code_folding: show
---

# Introduction


# Analysis set up

## Load libraries

```{r setup, message=FALSE}
library(dplyr, quietly=TRUE)
library(readr, quietly=TRUE)
library(purrr, quietly=TRUE)
library(stm, quietly=TRUE)
library(tidytext, quietly=TRUE)
library(ggplot2, quietly=TRUE)
library(quanteda, quietly=TRUE)
```

## Load data

List the files in the data area.  These were downloaded from Kaggle at https://www.kaggle.com/airbnb/boston.

```{r}
datapath <- '~/data/boston-airbnb-open-data/'
list.files(datapath)
```

Load the data into R using the `readr` package.

```{r warnings=FALSE}
listings <- readr::read_csv(file.path(datapath, 'listings.csv'))
dim(listings)
colnames(listings)
```

# Data preparation

## Select data of interest

Only require some of the fields for later analysis.  In particular we are interested in price and accomodation description.  We also only 

```{r}
stm_df <- listings %>%
  dplyr::filter(grepl('Entire', room_type)) %>%
  dplyr::transmute(id, score=log(as.numeric(gsub('\\$', '', price))), comments=description) %>%
  dplyr::filter(!is.na(score)) 
stm_df
```

## Tokenise and remove stopwords

Tidy the comments using the tidytext 'bag of words' approach bu tokenising and then removing stopwords with an anti-join.

```{r}
tidy_comments <- stm_df %>%
  unnest_tokens(word, comments) %>%
  anti_join(stop_words)
tidy_comments
```

## Examine the number of words extracted

We can count the number of words present following tokensiation:

```{r}
tidy_comments %>%
  count(word, sort=TRUE)
```

Then extract the most common 1000 words to be used in the analysis

```{r}
common_words <- tidy_comments %>%
  count(word, sort=TRUE) %>%
  top_n(1000,n) %>% dplyr::select(-n)
common_words
```

## Create a document term frequency matrix

The tidy comments data frame can be converted into a document term frequency matrix using the `tidytext::cast_dfm` function.  The object type is a `dfm` object from the `quanteda` package.  

```{r}
comments_dfm <- tidy_comments %>%
  inner_join(common_words) %>%
  count(id, word) %>%
  arrange(id) %>%
  tidytext::cast_dfm(id, word, n)
comments_dfm
```

## Make a metadata data frame

In order to include covariates in the Structural Topic Model, we also need to create a metadata data frame.  In this case the covariate of interest is the price of the listing.  Importantly the rownames of the metadata dataframe must be the document id (ie listing id in this case).

```{r}
comments_dfm_meta <- stm_df %>%
  dplyr::group_by(id, score) %>%
  dplyr::summarise(comments=dplyr::first(comments)) %>%
  ungroup() %>%
  dplyr::filter(id %in% docnames(comments_dfm)) %>%
  dplyr::arrange(id) %>%
  as.data.frame()
rownames(comments_dfm_meta) <- comments_dfm_meta$listing_id
```

## Create an stm corpus object

Now convert the dataframes in an stm corpus object

```{r}
comments_stmc <- asSTMCorpus(comments_dfm, data=comments_dfm_meta)
```

# Topic modelling

## Identify optimal value of K

### Set up search grid

Specify the range of topic numbers that we want to search.  Since we can parallise across up to 16 cores we will look at 30 different values of K - leaving one core free.

```{r}
n_topics <- seq(4,33,1)
length(n_topics)

```

### Run the grid search

Now run the grid search over the 30 values of K specified.

```{r searchK, eval=FALSE}
topic_store <- searchK(comments_stmc$documents,
  comments_stmc$vocab,
  K = n_topics,
  data = comments_stmc$data,
  prevalence = ~ score,
  max.em.its = 20,
  cores = 15)
```

### Evaluate the models generated

The stm package includes functionality to generate some useful diagnostic plots:

```{r eval=FALSE}
plot(topic_store)
```

Not very clear in this case, probably don't have sufficient observations, but 8 looks reasonable.

```{r}
best_k <- 8
```

## Fit the topic model

Now fit the topic model with the chosen number of topics.

```{r fit-model}
topic_model <- stm(comments_stmc$documents,
  comments_stmc$vocab,
  K = best_k,
  data = comments_stmc$data,
  prevalence = ~ score,
  max.em.its = 20,
  verbose=FALSE)
```

Generate a summary of the topic model.  For each topic, the topi words according to four different metrics are generated.  Top probability is just the most common words, the other metrics use different forms of weighting to pick up words that are more exlusive to the topic.  See the stm vignette and package documentation for more information.

```{r}
summary(topic_model)
```

## Estimate the effect of price

Now estimate the effect of the price covariate on the prevalence of each topic:

```{r est-effect}
est_effect <- estimateEffect(1:best_k ~ score, topic_model, meta=comments_dfm_meta)
```

